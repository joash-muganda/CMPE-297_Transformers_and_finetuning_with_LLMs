{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUDzgCqGxfx7"
      },
      "source": [
        "# NanoGPT Implementation - Training on Pride and Prejudice\n",
        "\n",
        "This notebook implements a small GPT model from scratch using PyTorch and trains it on Jane Austen's \"Pride and Prejudice\". Just run all cells in order - it will automatically:\n",
        "1. Download Pride and Prejudice from Project Gutenberg\n",
        "2. Train a small GPT model (takes about 5-10 minutes)\n",
        "3. Generate Jane Austen-style text\n",
        "\n",
        "No additional data or setup needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KwZptLhxfx-",
        "outputId": "e7433fb1-1ca2-4926-f13b-18f1d5a217ac"
      },
      "source": [
        "# Install required packages\n",
        "!pip install torch requests"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bX-Ce4uxfx_",
        "outputId": "dfe44a59-b7c5-46f7-d563-c07b0930f58a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import requests\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXFwGmHxfx_"
      },
      "source": [
        "## Configuration - Optimized for Pride and Prejudice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tnMHxv9BxfyA"
      },
      "source": [
        "class Config:\n",
        "    # Data parameters\n",
        "    batch_size = 32\n",
        "    block_size = 128  # Increased for better context in novel text\n",
        "\n",
        "    # Training parameters\n",
        "    max_iters = 3000  # Increased for better quality\n",
        "    eval_interval = 100\n",
        "    learning_rate = 3e-4\n",
        "    eval_iters = 100\n",
        "\n",
        "    # Model parameters\n",
        "    n_embd = 256     # Increased for richer representations\n",
        "    n_head = 8       # More attention heads\n",
        "    n_layer = 6      # More layers for complexity\n",
        "    dropout = 0.2\n",
        "\n",
        "    # Device configuration\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Dataset URL - Pride and Prejudice from Project Gutenberg\n",
        "    dataset_url = 'https://www.gutenberg.org/files/1342/1342-0.txt'\n",
        "\n",
        "config = Config()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9838SybxfyA"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YUbjzzdSxfyA"
      },
      "source": [
        "def download_dataset(url: str) -> str:\n",
        "    \"\"\"Download dataset from URL and return text content\"\"\"\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "\n",
        "    # Find the start and end of the actual book content\n",
        "    start = text.find(\"PRIDE AND PREJUDICE\")\n",
        "    end = text.find(\"End of the Project Gutenberg\")\n",
        "\n",
        "    # Extract just the book content\n",
        "    return text[start:end]\n",
        "\n",
        "def create_vocab(text: str) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    \"\"\"Create vocabulary mappings from text\"\"\"\n",
        "    chars = sorted(list(set(text)))\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "    return stoi, itos\n",
        "\n",
        "def encode(s: str, stoi: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"Encode string to list of integers\"\"\"\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l: List[int], itos: Dict[int, str]) -> str:\n",
        "    \"\"\"Decode list of integers to string\"\"\"\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, get_batch, config):\n",
        "    \"\"\"Estimate loss on train and validation sets\"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(config.eval_iters)\n",
        "        for k in range(config.eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5bAqu_WxfyA"
      },
      "source": [
        "## Data Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_KgYVPvkxfyB"
      },
      "source": [
        "class DataModule:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.train_data = None\n",
        "        self.val_data = None\n",
        "        self.stoi = None\n",
        "        self.itos = None\n",
        "        self.vocab_size = None\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Download and prepare the dataset\"\"\"\n",
        "        print('Downloading Pride and Prejudice...')\n",
        "        text = download_dataset(self.config.dataset_url)\n",
        "        print(f'Dataset size: {len(text)} characters')\n",
        "\n",
        "        # Show a sample of the text\n",
        "        print('\\nSample of the text:')\n",
        "        print(text[:500], '...\\n')\n",
        "\n",
        "        # Create vocabulary\n",
        "        self.stoi, self.itos = create_vocab(text)\n",
        "        self.vocab_size = len(self.stoi)\n",
        "        print(f'Vocabulary size: {self.vocab_size} unique characters')\n",
        "\n",
        "        # Encode full text\n",
        "        data = torch.tensor(encode(text, self.stoi), dtype=torch.long)\n",
        "\n",
        "        # Split into train and validation\n",
        "        n = int(0.9 * len(data))\n",
        "        self.train_data = data[:n]\n",
        "        self.val_data = data[n:]\n",
        "        print(f'Split sizes: {len(self.train_data)} train, {len(self.val_data)} validation')\n",
        "\n",
        "    def get_batch(self, split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Generate a small batch of data of inputs x and targets y\"\"\"\n",
        "        data = self.train_data if split == 'train' else self.val_data\n",
        "        ix = torch.randint(len(data) - self.config.block_size, (self.config.batch_size,))\n",
        "        x = torch.stack([data[i:i+self.config.block_size] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+self.config.block_size+1] for i in ix])\n",
        "        x, y = x.to(self.config.device), y.to(self.config.device)\n",
        "        return x, y"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-lCeJDixfyB"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RxKf2lmnxfyB"
      },
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "    def __init__(self, head_size: int, n_embd: int, block_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        # Weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "    def __init__(self, config, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([\n",
        "            Head(head_size, config.n_embd, config.block_size, config.dropout)\n",
        "            for _ in range(config.n_head)\n",
        "        ])\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Simple linear layer followed by non-linearity\"\"\"\n",
        "    def __init__(self, n_embd: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        head_size = config.n_embd // config.n_head\n",
        "        self.sa = MultiHeadAttention(config, head_size)\n",
        "        self.ffwd = FeedForward(config.n_embd, config.dropout)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"The main GPT language model\"\"\"\n",
        "    def __init__(self, config, vocab_size):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.config.device))\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Apply transformer blocks and final layer norm\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Compute loss if targets provided\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"Generate new tokens from the model\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context to block_size\n",
        "            idx_cond = idx[:, -self.config.block_size:]\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Focus on last time step\n",
        "            logits = logits[:, -1, :]\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append sampled index to running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNtPjtIGxfyC"
      },
      "source": [
        "## Training\n",
        "\n",
        "This will take about 10-15 minutes. You'll see the training progress every 100 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Laaxb5pxfyC",
        "outputId": "a0f0dd92-9930-48be-b3e5-777eab1517b3"
      },
      "source": [
        "print('Preparing data...')\n",
        "data_module = DataModule(config)\n",
        "data_module.prepare_data()\n",
        "\n",
        "print('\\nInitializing model...')\n",
        "model = GPT(config, data_module.vocab_size)\n",
        "model = model.to(config.device)\n",
        "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M')\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "print('\\nStarting training...')\n",
        "for iter in range(config.max_iters):\n",
        "    # Evaluate loss periodically\n",
        "    if iter % config.eval_interval == 0:\n",
        "        losses = estimate_loss(model, data_module.get_batch, config)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = data_module.get_batch('train')\n",
        "\n",
        "    # Evaluate loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print('Training complete!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data...\n",
            "Downloading Pride and Prejudice...\n",
            "Dataset size: 708409 characters\n",
            "\n",
            "Sample of the text:\n",
            "PRIDE AND PREJUDICE·\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Chapter I.]\n",
            "\n",
            "\n",
            "It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "However little known the feelings or views of such a man may be on his\n",
            "first entering a neighbourhood, this truth is so well fixed in the minds\n",
            "of the surrounding families, that he is considered as the rightful\n",
            "property of some one or other of their daughters.\n",
            "\n",
            "“My dear Mr. Bennet,” said his lady to him one day, “have you hea ...\n",
            "\n",
            "Vocabulary size: 89 unique characters\n",
            "Split sizes: 637568 train, 70841 validation\n",
            "\n",
            "Initializing model...\n",
            "Number of parameters: 4.81M\n",
            "\n",
            "Starting training...\n",
            "step 0: train loss 4.5917, val loss 4.5880\n",
            "step 100: train loss 2.4525, val loss 2.4686\n",
            "step 200: train loss 2.3902, val loss 2.4081\n",
            "step 300: train loss 2.3446, val loss 2.3692\n",
            "step 400: train loss 2.2723, val loss 2.2874\n",
            "step 500: train loss 2.1039, val loss 2.1129\n",
            "step 600: train loss 1.9626, val loss 1.9604\n",
            "step 700: train loss 1.8569, val loss 1.8645\n",
            "step 800: train loss 1.7693, val loss 1.7670\n",
            "step 900: train loss 1.6936, val loss 1.6999\n",
            "step 1000: train loss 1.6414, val loss 1.6546\n",
            "step 1100: train loss 1.5902, val loss 1.5976\n",
            "step 1200: train loss 1.5452, val loss 1.5527\n",
            "step 1300: train loss 1.5068, val loss 1.5194\n",
            "step 1400: train loss 1.4747, val loss 1.4887\n",
            "step 1500: train loss 1.4464, val loss 1.4648\n",
            "step 1600: train loss 1.4132, val loss 1.4384\n",
            "step 1700: train loss 1.3898, val loss 1.4221\n",
            "step 1800: train loss 1.3649, val loss 1.4022\n",
            "step 1900: train loss 1.3541, val loss 1.3834\n",
            "step 2000: train loss 1.3341, val loss 1.3619\n",
            "step 2100: train loss 1.3135, val loss 1.3530\n",
            "step 2200: train loss 1.3051, val loss 1.3392\n",
            "step 2300: train loss 1.2816, val loss 1.3245\n",
            "step 2400: train loss 1.2658, val loss 1.3143\n",
            "step 2500: train loss 1.2606, val loss 1.3000\n",
            "step 2600: train loss 1.2427, val loss 1.2860\n",
            "step 2700: train loss 1.2335, val loss 1.2841\n",
            "step 2800: train loss 1.2255, val loss 1.2739\n",
            "step 2900: train loss 1.2148, val loss 1.2666\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFivInjQxfyC"
      },
      "source": [
        "## Generate Jane Austen-style Text\n",
        "\n",
        "Let's see what our trained model can write in the style of Pride and Prejudice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rebdu1J2xfyC",
        "outputId": "7767b966-eb70-4c64-fcea-d773f59aea5c"
      },
      "source": [
        "print('Generating text...\\n')\n",
        "# Start with \"It is a truth universally acknowledged\" as context\n",
        "start_text = \"It is a truth universally acknowledged\"\n",
        "context = torch.tensor([encode(start_text, data_module.stoi)], dtype=torch.long, device=config.device)\n",
        "\n",
        "generated_text = decode(\n",
        "    model.generate(context, max_new_tokens=500)[0].tolist(),\n",
        "    data_module.itos\n",
        ")\n",
        "print(generated_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text...\n",
            "\n",
            "It is a truth universally acknowledged him but\n",
            "       in his differens would not here; almost it, and, Mr. Darcy characted\n",
            "her. HThe is verity were father, was less another expect to he lett in\n",
            "leaving rapets. She purson such and slike of the countrue! I or part for\n",
            "ever, Miss Darcy warmly to be continued her siters was childned on live\n",
            "it; and “by sink, you be know hasten rese idensify.”\n",
            "\n",
            "“The direct well,” said she she same inly my on morning, unly Georsible;\n",
            "to have conbuded but paces.” Colonel Fitzwitzwish it was nortain\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}